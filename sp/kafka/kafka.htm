---
layout: page
title: Using Kafka
description: Provides information about Kafka implementation, including benefits, installation, and configuration.
product: sp
category: gs
weight: 05
sub-nav-class: Kafka
type: page
nav-title: Using Kafka
---
<h1 id="top">Using Kafka</h1>
<p>Provides information about Kafka implementation, including benefits, installation, and configuration.</p>
<h4 class="stamp">Valid in Version: 2020.1.1 and later</h4>
<hr class="divide_hr" />



<h2 style="color: gray;">Table of Contents</h2>
<ol class="table_of_contents">
	<li><a href="#overview">Overview</a></li>
	<li><a href="#kafka_advantages">Advantages of Kafka implementation with the Akana Platform</a></li>
<li><a href="#kafka_deployment_rec">Kafka Deployment Recommendations</a></li>
<li><a href="#kafka_install">Installing Apache Kafka for testing purposes</a></li>
	<li><a href="#kafka_akana_install">Installing Akana Kafka features</a></li>
<li><a href="#kafka_config_properties">Configuration Properties for Kafka</a></li>
<li><a href="#topic_config">Topic Configuration</a></li>
</ol>
<hr class="divide_hr" />



<h2 id="overview">Overview</h2>
<p>The Akana Platform offers the option to use <a href="../../cm/learnmore/basics_glossary.htm#gl_kafka">Kafka</a> as an alternative for streaming audit data. If you have an implementation that's high-volume, with many or very large messages, you can use Kafka to more efficiently manage the traffic flow to help ensure the load on the API Gateway remains manageable. Audit data is channeled through Kafka and is then persisted through the RDBMS or in MongoDB. In general, the Gateway produces data into Kafka. Consumer containers will then, in turn, read the data.</p>
<p>Kafka is noted for its capabilities to support high throughput, scalability, safe storage of streams of data in a distributed and fault-tolerant cluster, and high availability across distributed clusters and geographical regions. It supports scalability with fault tolerance for heavy-traffic systems. Kafka is distributed, partitioned, and highly fault-tolerant.</p>
<p>Using Kafka as part of the API Platform requires separate installation and configuration of Kafka itself, including hardware requirements. See <a href="#kafka_akana_install">Installing Akana Kafka features</a>.</p>
<p>You also have the option, as part of your Kafka implementation, to use Elasticsearch for temporary storage of large messages. See <a href="#kafka_config_claim_check">Claim Check pattern (with Detailed Auditing policy)</a> below.</p>
<p>For general information about Kafka, see <a href="https://kafka.apache.org/" title="Link to Apache Kafka site" target="_blank">https://kafka.apache.org/</a> (external site).</p>
<p><a href="#top">Back to top</a></p>



<h2 id="kafka_advantages">Advantages of Kafka implementation with the Akana Platform</h2>
<p>By using Kafka in your implementation, you can benefit from the additional data processing capabilities of Kafka. Practical benefits:</p>
<ul>
	<li>Reduction of memory load on the Gateway&#8212;Help reduce the possibility of out of memory (OOM) problems with the Gateway. Using Kafka helps reduce memory load on the Gateways by moving usage data, particularly recorded messages, out of memory in a more timely manner.</li>
<li>Reduction of potential impact on other containers&#8212;In-memory usage data and metric data causing out of memory issues on the Gateways could also impact the Policy Manager containers which are the consumers of the data.</li>
<li>Safety of usage data&#8212;Helps prevent loss of usage data. Because of limitations relating to the synchronous REST APIs used to report usage and metrics, loss of usage data is possible in times of high volume. Of course, usage data is critical. Using Kafka is a way to implement a more efficient configuration, in scenarios where loss of critical usage data might be a possibility because of high volume and/or limited resources.</li>
<li>Kafka can help balance load in terms of CPU, memory, and persistent storage management across the system. For example, a pull model such as Kafka for usage data in Policy Manager allows for a more even load profile than the default push model through REST APIs.</li>
<li>Using Kafka can help reduce stress on both Gateway containers and Policy Manager containers. The availability and health of the Policy Manager containers is essential for the success of the Gateways.</li>
	<li>Using Kafka, you can use a percentage of the Heap instead of an arbitrary queue size number (the maximum number of messages that can be queued) for usage messages/bulk data.</li>
</ul>
<p><a href="#top">Back to top</a></p>



<h2 id="kafka_deployment_rec">Kafka Deployment Recommendations</h2>
<p>In this section:</p>
<ul>
<li><a href="#deploy_rec_version">Supported versions</a></li>
<li><a href="#deploy_rec_brokers">Brokers</a></li>
	<li><a href="#deploy_rec_zoo">Zookeeper instances</a></li>
	<li><a href="#deploy_rec_inst_consumer">Consumer instances</a></li>
	<li><a href="#deploy_rec_partition">Partitions</a></li>
	<li><a href="#deploy_rec_replication">Replication factor</a></li>
<li><a href="#deploy_rec_disc">Disk allocation</a></li>
<li><a href="#deploy_rec_monitoring">Monitoring</a></li>
	<li><a href="#deploy_rec_mirror">Cross-region mirroring</a></li>
</ul>


<h3 id="deploy_rec_version">Supported versions</h3>
<p>For Kafka supported version, refer to the System Requirements documentation. Go to <a href="../system_requirements/system_requirements_all.htm">System Requirements (all versions)</a> and choose your version. Then, go to the <strong>Kafka version</strong> section.</p>
<h3 id="deploy_rec_brokers">Brokers</h3>
<p>Recommendation: a minimum of 3 brokers with 4 vCPU and 16GB RAM each.</p>


<h3 id="deploy_rec_zoo">Zookeeper instances</h3>
<p>Recommendation: a minimum of 3 Zookeeper instances with 2 vCPU and 8GB RAM each.</p>
<h3 id="deploy_rec_inst_consumer">Consumer instances</h3>
<p>Recommendation: a minimum of 3 consumer instances with 4 vCPU and 16GB RAM each.</p>
<p>The number of consumers is usually proportionate to the number of partitions. The consumer instances should be increased based on the throughput that the Gateway containers are generating, so that the consumer throughput is balanced with the producer throughput.</p>


<h3 id="deploy_rec_partition">Partitions</h3>
<p>In general, use three times as many partitions as there are Gateways/producers. This is dependent on the number of consumer nodes (see above). A rule of thumb formula:</p>
<pre>Number of partitions &gt;= ND * 3</pre>
<p>For recommendations and formulas, refer to the Kafka documentation: <a href="https://kafka.apache.org/documentation/" title="Link to Kafka documentation" target="_blank">https://kafka.apache.org/documentation/</a>.</p>
<h3 id="deploy_rec_replication">Replication factor</h3>
<p>The maximum replication factor is:</p>
<pre>Number of replicas &lt;= number of brokers</pre>
<p>For example, in an environment with three brokers, two replications are recommended.</p>
<p>For more information, refer to the Kafka documentation, Replication section: <a href="https://kafka.apache.org/documentation/#replication" title="Link to Kafka documentation, Replication section" target="_blank">https://kafka.apache.org/documentation/#replication</a>.</p>


<h3 id="deploy_rec_disc">Disk allocation</h3>
<p>Recommendation:</p>
<ul>
<li>General purpose SSD</li>
<li>10 MB/s data flow</li>
	<li>Disk size should be allocated according to retention time and usage.</li>
</ul>


<h3 id="deploy_rec_monitoring">Monitoring</h3>
<p>In any production deployment it is expected that there will be a system-wide monitoring of Kafka.</p>
<h3 id="deploy_rec_mirror">Cross-region mirroring</h3>
<p>Cross-region mirroring (replication) is out of scope in the current implementation of Kafka.</p>
<p><a href="#top">Back to top</a></p>



<h2 id="kafka_install">Installing Apache Kafka for testing purposes</h2>
<p>You might want to install a single-broker Kafka on a standalone container, locally, perhaps for testing purposes. Of course for a production environment you would need more, but for testing purposes you could set up Kafka using the Apache Kafka Quickstart Guide: <a href="http://kafka.apache.org/quickstart" title="http://kafka.apache.org/quickstart" target="_blank">http://kafka.apache.org/quickstart</a>.</p><p><a href="#top">Back to top</a></p>



<h2 id="kafka_akana_install">Installing Akana Kafka features</h2>
<p>You'll need to install Kafka features in each container:</p>
<ul>
<li><a href="#kafka_install_nd">Installing Kafka in the Network Director container</a></li>
<li><a href="#kafka_install_pm_cm">Installing Kafka in consumer containers</a></li>
</ul>
<p>In each container, install the following features, available in the Akana Administration Console: Available Features &gt; Plug-In:</p>
<ul>
<li>Akana Kafka Support for Network Director</li>
<li>Akana Kafka Support for Policy Manager</li>
</ul>


<h3 id="kafka_install_nd">Installing Kafka in the Network Director container</h3>
<p>The Network Director writes to Kafka.</p>
<p>In each Network Director container, install the <strong>Akana Kafka Support for Network Director</strong> feature. Follow the steps below.</p>

<h4 id="proc_kafka_install_nd">To install Kafka on the Network Director container</h4>
<ol>
<li>Log in to the Akana Administration Console for the Network Director container.</li>
<li>Go to the <strong>Available Features</strong> tab and then, at the <strong>Filter</strong> drop-down on the right, click <strong>Plug-In</strong>.</li>
<li>Check the box next to the <strong>Akana Kafka Support for Network Director</strong> feature and click <strong>Install Feature</strong>.</li>
<li>At the <strong>Resolution Report</strong> page, click <strong>Install Feature</strong>.</li>
<li>At the <strong>Installation Complete</strong> page, click <strong>Configure</strong>. The <strong>Configure Kafka Brokers for Auditing</strong> page appears, as shown below.
<p><img src="images/aac_configure_kafka_brokers_for_auditing.png" alt="Akana Admin Console: Configure Kafka Brokers for Auditing page" /></p>
</li>
<li>Enter the information needed for Network Director to connect to the Kafka Brokers. Note:
<ul>
<li>Format is {hostname}:{port}. For example, <strong>localhost:9092</strong>.</li>
<li>For multiple bootstrap servers, use a comma separator. For example, <strong>hostname1:9092,hostname2:9093,hostname3:9094.</strong></li>
</ul>
</li>
<li>Click <strong>Finish</strong>.</li>
<li>At the prompt, restart the system.</li>
</ol>
<p>The next step is to turn on the Kafka writers.</p>


<h3 id="kafka_install_pm_cm">Installing Kafka in consumer containers</h3>
<p>The Consumer container (Policy Manager or Community Manager) reads from Kafka.</p>
<p>In each Policy Manager or Community Manager container, install the <strong>Akana Kafka Support for Policy Manager</strong> feature. Follow the steps below.</p>

<h4 id="proc_kafka_install_pm_cm">To install Kafka on a Policy Manager or Community Manager container</h4>
<ol>
<li>Log in to the Akana Administration Console for the consumer container.</li>
<li>Go to the <strong>Available Features</strong> tab and then, at the <strong>Filter</strong> drop-down on the right, click <strong>Plug-In</strong>.</li>
<li>Check the box next to the <strong>Akana Kafka Support for Policy Manager</strong> feature and click <strong>Install Feature</strong>.</li>
<li>At the <strong>Resolution Report</strong> page, click <strong>Install Feature</strong>.</li>
<li>At the <strong>Installation Complete</strong> page, click <strong>Configure</strong>. The <strong>Configure Kafka Brokers for Auditing</strong> page appears. For an illustration, see above.</li>
<li>Enter the information needed for the consumer container to connect to the Kafka Brokers. Note:
<ul>
<li>Format is {hostname}:{port}. For example, <strong>localhost:9092</strong>.</li>
<li>For multiple bootstrap servers, use a comma separator. For example, <strong>hostname1:9092,hostname2:9093,hostname3:9094.</strong></li>
</ul>
</li>
<li>Click <strong>Finish</strong>.</li>
<li>At the prompt, restart the system.</li>
</ol>
<p>The next step is to turn on the Kafka readers on each consumer container.</p>



<h2 id="kafka_config_properties">Configuration Properties for Kafka</h2>
<p>When you install the Kafka feature, either in a Network Director container or a Policy Manager or Community Manager container, additional configuration properties become available, as shown below.</p>
<p>In the Akana Administration Console for the container, click the <strong>Configuration</strong> tab.</p>
<p>These two sets of configuration properties affect Kafka:</p>
<ul>
	<li><a href="#config_com_akana_kafka">Configuration category: com.akana.kafka</a></li>
	<li><a href="#config_com_akana_monitor_kafka">Configuration category: com.akana.monitor.kafka</a></li>
</ul>


<h3 id="config_com_akana_kafka">Configuration category: com.akana.kafka</h3>
<p>The three values shown below are set when you install the Kafka feature.</p>
<p>In this configuration category, you can add additional properties and values supported by Kafka. There are many supported configuration properties, as covered in the Apache Kafka documentation, Configuration section: <a href="https://kafka.apache.org/documentation/#configuration" title="https://kafka.apache.org/documentation/#configuration" target="_blank">https://kafka.apache.org/documentation/#configuration</a>.</p>
<p>For example, you could add the <strong>acks</strong> property to configure "the number of acknowledgments the producer requires the leader to have received before considering a request complete,&quot; along with <strong>min.insync.replicas</strong> to specify &quot;the minimum number of replicas that must acknowledge a write for the write to be considered successful.&quot; Or you could add the <strong>ssl.truststore.location</strong> and <strong>ssl.truststore.password</strong> properties to set up a trust store.</p>
<p><img src="images/aac_config_com_akana_kafka.png" alt="Akana Admin Console: Configuration properties for Kafka, com.akana.kafka" /></p>
<dl>
<dt>bootstrap.servers</dt>
<dd>The address for the bootstrap servers, which you set up as part of installing the Kafka feature. This is the <strong>bootstrap.servers</strong> Kafka configuration property. Format is {hostname{:{port}, with comma separators for multiple values. For an example see <a href="#proc_kafka_install_nd">To install Kafka on the Network Director container</a>, or refer to the Kafka documentation for the <strong>bootstrap.servers</strong> property: <a href="https://kafka.apache.org/documentation/#bootstrap.servers" title="https://kafka.apache.org/documentation/#bootstrap.servers" target="_blank">https://kafka.apache.org/documentation/#bootstrap.servers</a>.</dd>

<dt>role</dt>
<dd>The role of Kafka in this instance. Defaults to <strong>monitoring</strong>, the only valid value.</dd>

<dt>service.factoryPid</dt>
<dd>Defaults to the configured value.</dd>

</dl>

<h3 id="config_com_akana_monitor_kafka">Configuration category: com.akana.monitor.kafka</h3>
<p><img src="images/aac_config_com_akana_monitor_kafka.png" alt="Akana Admin Console: Configuration properties for Kafka, com.akana.monitor.kafka" /></p>
<p>Set the following properties:</p>
<dl>
<dt>reader values:</dt>
<dd>For the Policy Manager and Community Manager containers, or any other Akana Kafka consumer container, set the value for all four <strong>reader</strong> properties to <strong>true</strong>:
<ul>
<li>kafka.bulk.reader.enable</li>
<li>kafka.rollup.reader.enable</li>
<li>kafka.transaction.reader.enable</li>
<li>kafka.usage.reader.enable</li>
</ul></dd>

<dt>writer values</dt>
<dd>For the Network Director containers, set the value for all four <strong>writer</strong> properties to <strong>true</strong>:
<ul>
<li>kafka.bulk.writer.enable</li>
<li>kafka.rollup.writer.enable</li>
<li>kafka.transaction.writer.enable</li>
<li>kafka.usage.writer.enable</li>
</ul></dd>
</dl>
<p><a href="#top">Back to top</a></p>




<h2 id="topic_config">Configuration</h2>
<p>In this section:</p>
<ul>
<li><a href="#kafka_config_topic">Topic Configuration</a></li>
<li><a href="#kafka_config_server_ts">Server Configuration and Trust Store</a></li>
<li><a href="#kafka_config_claim_check">Claim Check pattern (with Detailed Auditing policy)</a></li>
</ul>


<h3 id="kafka_config_topic">Topic Configuration</h3>
<p>When you install the Kafka feature, the platform includes recommended topic names. If you accept these recommendations, no other Kafka usage configuration is needed.</p>
<p>If you choose to use different topic names, you'll need to update the configuration on all Gateways and on all containers on which the Akana Kafka Consumers feature is installed.</p>
<p>The default topic names are:</p>
<ul>
<li>container.usage</li>
<li>container.bulk</li>
<li>container.rollup</li>
<li>container.transaction</li>
</ul>
<p>You'll need to create these topics in the Broker as part of setup, using the Create Topic script: see <a href="http://kafka.apache.org/quickstart" title="http://kafka.apache.org/quickstart" target="_blank">http://kafka.apache.org/quickstart</a>, Step 3, Create a Topic to Store Your Events.</p>


<h3 id="kafka_config_server_ts">Server Configuration and Trust Store</h3>
<p>For information about the server configuration and trust store properties, refer to the Apache Kafka documentation, Configuration section: <a href="https://kafka.apache.org/documentation/#configuration" title="https://kafka.apache.org/documentation/#configuration" target="_blank">https://kafka.apache.org/documentation/#configuration</a>.</p>
<p>You can add these properties in the com.akana.kafka configuration. See <a href="#config_com_akana_kafka">Configuration category: com.akana.kafka</a>.</p>


<h3 id="kafka_config_claim_check">Claim Check pattern (with Detailed Auditing policy)</h3>
<p>The Akana Platform Kafka implementation allows you to use Elasticsearch for temporary storage of large messages (&quot;claim check&quot; system) to help optimize traffic handling and ease the burden on the Gateway. The message body and headers are stored in Elasticsearch, with a unique ID. The only data that is indexed is the ID; message and header content are not indexed, which means that the message size does not increase the processing burden significantly. If you use the Detailed Auditing policy on your messages, with a record size (payload and header) exceeding 20 KB, you might choose this option. However, note that we do not recommend using the Detailed Auditing policy habitually in a runtime environment because of the increased load to the system. In most cases, the Basic Auditing policy is all that's needed.</p>
<p><strong>Note</strong>: This Elasticsearch implementation is separate from the Elasticsearch implementation used for indexing in the Community Manager developer portal.</p>

<h4 id="proc_kafka_es_enable">To enable Elasticsearch for use with Kafka</h4>
<p>Follow the steps below for each container that is running Kafka.</p>
<ol>
<li>Log in to the Akana Administration Console for the container.</li>
<li>Click the Configuration tab and then, under Configuration Categories, go to <strong>com.akana.monitor.kafka</strong>.</li>
<li>Change the value of the <strong>kafka.bulk.writer.selector</strong> property from <strong>json</strong> to <strong>elastic</strong>.</li>
<li>In the <strong>es.client.hosts</strong> property, provide the Elasticsearch host in the format http://{hostname}:{port). For example: <strong>http://localhost:9200</strong>.</li>
<li>Click <strong>Finish</strong>.</li>
</ol>
<p><a href="#top">Back to top</a></p>



<hr class="divide_hr" />



<h2 id="related_topics">Related Topics</h2>
<div class="relatedlinks" id="install">
<ul>
	<li><a href="../upgrade/upgrading_akana_api_platform_all.htm">Upgrading the Akana API Platform (all versions)</a></li>
	<li><a href="../upgrade/upgrading_min_downtime.htm">Use Case: Upgrading with Minimum Downtime</a></li>
	<li><a href="../upgrade/upgrading_micro.htm">Minor-Version Update Notes</a></li>
</ul>
<ul>
	<li><a href="../platform_install/installing_akana_api_platform_all.htm">Installing the Akana API Platform (all versions)</a></li>
	<li><a href="../platform_install/implementation_planning.htm">Planning Your Implementation</a></li>
	<li><a href="../platform_install/install_cm_jython_script.htm">Community Manager Tenant Installation Jython Script</a></li>
	<li><a href="../elasticsearch/install_es_config.htm">Installing and Configuring Elasticsearch</a></li>
	<li><a href="../elasticsearch/install_es_secure_tc.htm">Configuring Elasticsearch with Security</a></li>
</ul>
</div>